# ğŸ–ï¸ Sign Language Recognition System

## ğŸ“‹ Project Description

This project aims to create a **real-time sign language recognition system** using machine learning techniques. The system bridges the communication gap between individuals with hearing impairments and the general population by converting sign language gestures into spoken or written language.

### ğŸš€ Objective
Develop a system that accurately and swiftly translates sign language motions into text or speech in **real-time**, making communication more accessible for everyone.

### ğŸ§© Problem Statement
People with hearing impairments face difficulties communicating with those who donâ€™t understand sign language. This project focuses on using **machine learning** to solve this issue by recognizing hand gestures and converting them into written or spoken language.

---

## ğŸ“š Dataset

The dataset for this project is sourced from **Kaggle**, specifically the [ASL Alphabet Dataset](https://www.kaggle.com/grassknoted/asl-alphabet). It contains labeled images of the American Sign Language (ASL) alphabet to train and test the model.

---

## ğŸ› ï¸ Technologies Used

This project utilizes several cutting-edge technologies for image recognition and machine learning:

- **[Scikit-learn](https://scikit-learn.org/)**: Simple and efficient tools for data mining and data analysis, built on NumPy, SciPy, and Matplotlib.
- **[OpenCV](https://opencv.org/)**: Open-source computer vision library, providing tools for computer vision and image processing.
- **[TensorFlow](https://www.tensorflow.org/)**: Open-source machine learning framework for building and deploying machine learning models.
- **[Keras](https://keras.io/)**: High-level neural networks API, enabling fast and easy prototyping.

---

## ğŸ› ï¸ Solution Workflow

1. **Preprocessing**: 
   - Gray scale conversion
   - Resizing images
   - Data type conversion
2. **Model Selection**: 
   - Different models are explored to find the best performing one.
3. **Real-time Recognition**: 
   - A system is built to capture and identify sign language gestures from webcam or other input devices in real-time.

---

## ğŸ”§ Training Process

During training, the model goes through several **epochs**, each representing a complete pass through the dataset. The model's parameters are adjusted after each epoch to improve prediction accuracy.

---

## ğŸ“ˆ Future Work

- **Improve Real-Time Accuracy**: Enhance the precision of live gesture predictions.
- **Speech Functionality**: Add speech output for recognized gestures.

---

## ğŸ‘¥ Team Members

- **Yoosha Mirza (E22CSEU0046)**: Dataset Selection, Model Creation
- **Sarthak Chauhan (E22CSEU0056)**: Model Selection, Model Creation
- **Siddhart Patel (E22CSEU0044)**: Model Selection, Model Creation

---

## ğŸ“ Contact Information

If you have any questions or suggestions, feel free to reach out!

---

